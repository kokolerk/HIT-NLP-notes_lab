# 12.词和文档表示和相似度计算

## 词的表示

- 独热表示

  - 无需多说了吧……相信小仙女你已经会了

- 词频-逆文档频率

  - $f_{ij}表示词t_i出现在文档d_j里面的频率$

    $TF_{ij}=\frac{f_{ij}}{max_{k}f_{kj}}$

    $n_i表示含有词t_i的文档数，N为文档总数，IDF_i=log\frac{N}{n_i}$

    $TF-IDF分数 \\\omega_{ij}=TF_{ij}\times IDF_j$

- 离散表示问题：语义鸿沟——无法计算相近语义；高维稀疏

- 分布式表示

  - 利用邻居表示，纬度通常设定为25～100
  - 潜在语义索引
    - 基于词和文档的共现关系：词——文档，有为1（或者频数），没有为0，
    - SVD降低纬度
    - 缺点：计算复杂度大，引入新词或者新文档代价大

- 词嵌入表示

  - 直接学习词的低维稠密的向量表示

    - word2vec

      - CBOW:上下文预测目标

      - skip-gram：目标预测上下文

        - PPT22页图
        - input $\to$ hidden $\to$Output; input大小为1x V;output大小也为1x V

      - 优化策略：负采样，层次化softmax

      - ==重点：skip-gram with negative sampling（SGNS）==

        - 正样本：（target，neighbor）窗口大小为+/-2；负样本：（target，随机选的词）

        - 训练一个logisticsRregression，训练分类器，区分正负样本；将权重作为embeddings

        - $P(+|t,c)$,$P(-|t,c)=1-P(+|t,c)$利用相似度计算+sigmoid函数，把相似度转化为概率值：$P(+|t,c)=\frac{1}{1+e^{-t·c}}$，$P(1|t,c)=\frac{e^{-t·c}}{1+e^{-t·c}}$

        - 一般300维词向量，随机初始化，权重也随机初始化，训练过程（梯度下降）中，使得正例相似度尽可能大，负例相似度尽可能小

          优化目标：$max\sum_{(t,c)\in +}P(+|t,c)+\sum_{(t,c)\in -}P(-|t,c)$ ,最后得到两个矩阵，target words；context words

        - 词向量评价方法：

          - 与人工打分的相似度进行对比
          - 类比计算

    - glove：利用共现概率比

    - fasttext：

      - 考虑单词的形态（利用单词的字符n-gram表示一个词）；
      - 可以解决未登录词；
      - 基于skip-gram模型
      - 可以显著提高==句法单词类比，词相似度计算==任务性能好，但是在==语义类比==任务上性能下降

  - 问题：静态的（无法随语境变化），多义词一词多义无法解决，不能有效区分反义词

## 文档表示

- 词袋表示
  - 每个词的独热之和表示
- 潜在语义索引（狄利克雷分布）
- 主题模型（不要求搞懂）
- 基于深度学习的文档表示
  - 以词向量为基础
  - 深度置信网络
  - 卷积神经网络
  - 循环神经网络
  - 预训练语言模型

## 文本相似度计算

- 距离度量
  - 距离度量的公理
    - 非负
    - $d(x,y)=0 ,if \space x=y$
    - $d(x,y)=d(y,x)$
    - 三角不等式：$d(x,y)\le d(x,z)+d(z,y)$
  - 欧拉距离
    - l2，l1, l∞，
  - 非欧拉距离
    - jaccard：$sim(c1,c2)=|C_1\cap C_2|/|C_1\cup C_2|$
    - Cosine:$cos(\theta)=\frac{p_1·p_2}{|p_1||p_2|}$
    - Edit（编辑距离：增加，删除之后相同）：$d(x,y)=|x|+|y|-2|LCS(x,y)|,LCS(x,y)为最长的相同子序列$
    - variant edit（增加，删除，==变换==之后相同）
    - hamming距离（不同的位数）

