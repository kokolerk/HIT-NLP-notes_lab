## 中文分词

### 基于n元文法的分词

- 推导
- 一元文法/2元文法：
  - 难在参数平滑，参数爆炸

### 基于HMM的分词/词性标注一体化（模型）

- 推导：发射概率，转移概率
- viterbi算法

### 由字构词的汉语分词方法

- 对每个字进行标记——比如BMES

- 分类
  - 生成方法，可以理解为HMM模型，N元文法之类的，由前面的n个字生成第n+1个字：$\omega_{seg}=argmax_{\omega_{seg}}P(\omega|c_1c_2……c_n)$
  - 判别式方法，用特征来进行分词，ME，CRF等都是基于此：$P（t_1t_2……t_n|c_1c_2……c_n）=\prod P(t_k|t_1t_2……t_{k-1},c_{k-2}c_{k-1}c_{k}c_{k+1}c_{k+2})$其中t为状态序列（BMSE），c为字，$c_{k-2}c_{k-1}c_{k}c_{k+1}c_{k+2}$为特征，取的是大小为2的窗口。

### 汉语分词的后处理方法

基于转换错误驱动的标注学习

- 转换的两个组成部分：重写规则 +触发环境 

- 学习算法：选择最佳的转换                                                                            

  

### 中文未登录词识别

- 类型
- 依据
  - 内部构成规律（用词规律）
  - 外部环境（上下文）
  - 重复出现规律
- 例子：中文人名识别
  - 一般方法：转换为标注问题：BEIO
  - 标注采用：HMM,ME,MEMM（最大熵马尔可夫模型）,CRF